<!-- index.html -->
<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2BTE47V7K9"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2BTE47V7K9');
</script>

    <meta charset="utf-8">
    <meta name="google-site-verification" content="ihX6_5xF0HQU_0CWxKcwNsK4fDU6nFh4Je3w560ai_Y" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Kohei Uehara's website</title>
    <base href="/" />
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Electrolize" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500">
    <link rel="stylesheet" href="index.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

<!--    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.1.1/css/all.css"-->
<!--        integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">-->
    <script src="https://kit.fontawesome.com/9141dacc99.js" crossorigin="anonymous"></script>
</head>

<body>
    <!-- <div id="msg">loading...</div> -->
    <div id="main" class="container">
        <h2 class="page-title">Kohei Uehara's Website</h2>
        <hr />
        <h5>About me</h5>
            I am Kohei Uehara (上原 康平), an Assistant Professor at <a href="https://www.mi.t.u-tokyo.ac.jp/en/">Machine Intelligence Lab</a>, the University of Tokyo.
            I'm also working as a part-time researcher at <a href="https://www.miraikan.jst.go.jp/research/AccessibilityLab/">Accessibility Lab</a>, Miraikan (The National Museum of Emerging Science and Innovation).
            My research interest focuses on machine learning across vision and language, Large Language Models (LLMs), Accessibility, and Human-Computer Interaction (HCI).
        <hr />

        <h5>Current Positions</h5>
            <ul>
                <li>Assistant Professor, <a href="https://www.mi.t.u-tokyo.ac.jp/en/">Machine Intelligence Lab.</a>, Research Center for Advanced Science and Technology (RCAST), The University of Tokyo</li>
                <li>Part-Time Researcher, <a href="https://www.miraikan.jst.go.jp/research/AccessibilityLab/">Accessibility Lab.</a>, Miraikan (The National Museum of Emerging Science and Innovation)</li>
                <li>Visiting Researcher, <a href="https://www.riken.jp/en/research/labs/aip/goalorient_tech/machine_intell_med_eng/index.html">Machine Intelligence for Medical Engineering Team</a>, RIKEN</li>
            </ul>
        <hr />

        <h5>Education</h5>

            <ul>
                <li>April 2020 - March 2023 : Ph. D. student, Information Science and Technology, The University of Tokyo. (Advisor: Prof. Tatsuya Harada)</li>
                <li>April 2018 - March 2020 : Master’s student, Information Science and Technology, The University of Tokyo. (Advisor: Prof. Tatsuya Harada)</li>
                <li>April 2014 - March 2018 : Undergraduate student, Mechano-Informatics, The University of Tokyo. (Advisor: Prof. Tatsuya Harada)</li>
            </ul>
        <hr />
        <h5>Publications</h5>
        <h6>Journal and International Conference</h6>
            <ul class="widelist">
                <li>
                    <span class="badge badge-info">NEW</span> Kohtaro Tanaka, <u>Kohei Uehara</u>, Lin Gu, Yusuke Mukuta, Tatsuya Harada. <strong>Content-Specific Humorous Image Captioning Using Incongruity Resolution Chain-of-Thought</strong>.
                    In Findings of the Association for Computational Linguistics (NAACL Findings), 2024
                </li>
                <li>
                    <u>Kohei Uehara</u> and Tatsuya Harada. <strong>Learning by Asking Questions for Knowledge-Based Novel Object Recognition</strong>. International Journal of Computer Vision (IJCV), 2024
                    <a href="https://link.springer.com/article/10.1007/s11263-023-01976-7">[Paper]</a>
                    <a href="https://uehara-mech.github.io/learning-by-asking">[Project Page]</a>
                </li>
                <li><u>Kohei Uehara</u> and Tatsuya Harada. <strong>K-VQG: Knowledge-aware Visual Question Generation for Common-sense Acquisition</strong>. WACV, 2023.
                    <a href="https://openaccess.thecvf.com/content/WACV2023/html/Uehara_K-VQG_Knowledge-Aware_Visual_Question_Generation_for_Common-Sense_Acquisition_WACV_2023_paper.html">[Paper]</a> <a href="https://uehara-mech.github.io/kvqg">[Project Page]</a>
                </li>
                <li><u>Kohei Uehara</u>, Nan Duan, Tatsuya Harada.
                    <strong>Learning to Ask Informative Sub-Questions for Visual Question Answering</strong>.
                    5th Multimodal Learning and Applications Workshop (CVPR 2022, Workshop), 2022.
                    <a href="https://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Uehara_Learning_To_Ask_Informative_Sub-Questions_for_Visual_Question_Answering_CVPRW_2022_paper.html">[Paper]</a>
                </li>
                <li><u>Kohei Uehara</u>†, Yusuke Mori† (†equal contribution), Yusuke Mukuta and Tatsuya Harada.
                    <strong>ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer</strong>.
                    The 1st International Workshop on Multimodal Understanding for the Web and Social Media (WWW 2022, Workshop), 2022.
                    <a href="https://arxiv.org/abs/2202.07305">[Paper]</a></li>
                <li><u>Kohei Uehara</u>, Tatsuya Harada. <strong>Unsupervised Keyword Extraction for Full-sentence VQA.</strong> First International Workshop on Natural Language Processing Beyond Text with EMNLP 2020 (NLPBT2020), 2020. <a href="https://www.aclweb.org/anthology/2020.nlpbt-1.6/">[Paper]</a>
                <li>Sho Maeoki, <u>Kohei Uehara</u>, Tatsuya Harada. <strong>Interactive Video Retrieval with Dialog.</strong> CVPR 2020 Workshop on Multimodal Learning, 2020. <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w56/Maeoki_Interactive_Video_Retrieval_With_Dialog_CVPRW_2020_paper.html">[Paper]</a></li>
                <li><u>Kohei Uehara</u>, Antonio Tejero-de-Pablos, Yoshitaka Ushiku and Tatsuya Harada. <strong>Visual Question Generation for Class
                Acquisition of Unknown Objects.</strong> The 15th European Conference on Computer Vision (ECCV2018), 2018. <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Kohei_Uehara_Visual_Question_Generation_ECCV_2018_paper.html">[Paper]</a></li>
            </ul>
        <h6>Domestic Conference</h6>
        <ul class="widelist">
            <li>森 友亮†, <u>上原康平</u>† († equal contribution), 原田達也.  <strong>視覚・言語融合 Transformer モデルによる画像からの物語文生成</strong>. CAI+CAI first workshop（言語処理学会第27回年次大会 ワークショップ）, 2021.
                <a href="https://sites.google.com/view/cai-workshop">[Paper]</a>
            </li>
        </ul>
        <h6>Others</h6>
        <ul class="widelist">
            <li><span class="badge badge-info">NEW</span> Masaki Kuribayashi, <u>Kohei Uehara</u>, Allan Wang, Daisuke Sato, Simon Chu, Shigeo Morishima.
                <strong>Memory-Maze: Scenario Driven Benchmark and Visual Language Navigation Model for Guiding Blind People</strong>. arXiv, 2024.
                <a href="https://arxiv.org/abs/2405.07060">[Paper]</a>
            </li>
            <li><u>Kohei Uehara</u>, Nabarun Goswami, Hanqin Wang, Toshiaki Baba, Kohtaro Tanaka, Tomohiro Hashimoto, Kai Wang, Rei Ito, Takagi Naoya, Ryo Umagami, Yingyi Wen, Tanachai Anakewat, Tatsuya Harada.
                <strong>Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation</strong>. arXiv, 2024.
                <a href="https://arxiv.org/abs/2401.10005">[Paper]</a>
            </li>
        </ul>
        <hr />
        <h5>Competitions</h5>
            
            <ul>
                <li><strong>The 5th place in the Visual Question Answering (VQA) Challenge 2018 in CVPR2018.</strong> Mikihiro Tanaka, Atsuhiro Noguchi,
                <u>Kohei Uehara</u>, Lisa Kawai, Yoshitaka Ushiku, Tatsuya Harada
                    <a href="https://visualqa.org/challenge_2018.html">[competition page]</a>
                </li>
            </ul>
        <hr />
        <h5>Lectures</h5>
            <ul>
                <li>Intelligent Informatics - Graduate School of Information Science and Technology, The University of Tokyo, June 6, 2024</li>
                <iframe class="speakerdeck-iframe" frameborder="0" src="https://speakerdeck.com/player/5e03f9496fa24223b6b11f31bbf30ca2" title="自然言語とVision&amp;Language" allowfullscreen="true" style="border: 0px; background: padding-box padding-box rgba(0, 0, 0, 0.1); margin-top: 20px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 10px; width: 50%; height: auto; aspect-ratio: 560 / 315;" data-ratio="1.7777777777777777"></iframe>
            </ul>
        <h5>Invited Talks</h5>
            <ul>
                <li><u>Kohei Uehara</u>, Antonio Tejero-de-Pablos, Yoshitaka Ushiku and Tatsuya Harada. Visual Question Generation for
                    Class
                    Acquisition of Unknown Objects. <strong>FIT</strong>, 2019. <a href="https://www.ipsj.or.jp/event/fit/fit2019/">[Program]</a>
                </li>
                <li><u>Kohei Uehara</u>, Antonio Tejero-de-Pablos, Yoshitaka Ushiku and Tatsuya Harada. Visual Question Generation for Class
                Acquisition of Unknown Objects. <strong>MIRU</strong>, 2019. <a href="http://cvim.ipsj.or.jp/MIRU2019/">[Program]</a>
                </li>
                <li><u>Kohei Uehara</u>, Antonio Tejero-de-Pablos, Yoshitaka Ushiku and Tatsuya Harada. Visual Question Generation for Class
                Acquisition of Unknown Objects. <strong>PRMU</strong>, 2019. <a href="https://www.ieice.org/ken/program/index.php?tgs_regid=b89c5c2c61be284ce9fca177314c50a8b6026a6f4868fe70c49186a457bbd58e&tgid=IEICE-PRMU">[Program]</a></li>
            </ul>
        <hr />
        <h5>Work Experiences</h5>
            <ul>
                <li>April 2023 - Current: The University of Tokyo, Assistant Professor</li>
                <li>April 2021 - July 2021: NVIDIA, Research Internship</li>
                <li>February 2019 - April 2019 : LINE Corporation, Machine Learning Engineer, Part time job</li>
                <li>August 2018 : Mercari, Inc. Machine Learning Engineer Internship</li>
            </ul>
        <hr />
        <h5>Grants & Fellowships</h5>
        <ul>
            <li>January 2021 - December 2021 : Microsoft Research Asia Collaborative Research for Ph.D. Student 2021 (D-CORE 2021)</li>
            <li>April 2020 -  March 2023 : Japan Society for the Promotion of Science (JSPS) Research Fellowship for Young Scientists (DC1)</li>
        </ul>
        <hr />
        <h5>Professional Activities</h5>
        <ul>
            <li>Reviewer : ICCV, CVPR, ECCV, WACV, AAAI, NeurIPS, etc.</li>
<!--            <li>Program Committee : 2nd International Workshop on NLP Beyond Text, NLPBT2021 <a-->
<!--                href="https://sites.google.com/view/nlpbt-2021">[Link]</a></li>-->
<!--            <li>Program Committee : 1st International Workshop on NLP Beyond Text, NLPBT2020 <a-->
<!--                href="https://sites.google.com/view/nlpbt-2020">[Link]</a></li>-->
<!--            <li>Program Committee : 4th MUltimodal Learning and Applications Workshop, MULA2021 <a-->
<!--                href="https://mula-workshop.github.io/index_2021.html">[Link]</a></li>-->
        </ul>
        <hr />
        <h5>Links</h5>
            <a href="https://scholar.google.com/citations?user=yZFVY5cAAAAJ&hl=en">Google Scholar Citations</a>
            <br/>
            <div class="links">
                <a href="https://github.com/uehara-mech" class="link-icon"><i class="fa-brands fa-github-square fa-2x"></i></a>
                <a href="https://twitter.com/oldsea00731" class="link-icon"><i class="fa-brands fa-twitter-square fa-2x"></i></a>
                <a href="https://www.facebook.com/profile.php?id=100028442884186" class="link-icon"><i class="fa-brands fa-facebook-square fa-2x"></i></a>
                <a href="https://www.linkedin.com/in/kohei-uehara-58288b176/" class="link-icon"><i class="fa-brands fa-linkedin fa-2x"></i></a>
                <a href="https://speakerdeck.com/kuehara"><i class="fa-brands fa-speaker-deck fa-2x link-sd"></i></a>
            </div>
            
        <hr />
        <div class="update">
            last update: June 14, 2024
        </div>



    </div>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>
</body>

</html>