<!-- index.html -->
<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2BTE47V7K9"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2BTE47V7K9');
</script>

    <meta charset="utf-8">
    <meta name="google-site-verification" content="ihX6_5xF0HQU_0CWxKcwNsK4fDU6nFh4Je3w560ai_Y" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Kohei Uehara's website</title>
    <base href="/" />
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Electrolize" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500">
    <link rel="stylesheet" href="index.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

<!--    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.1.1/css/all.css"-->
<!--        integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">-->
    <script src="https://kit.fontawesome.com/9141dacc99.js" crossorigin="anonymous"></script>
</head>

<body>
    <!-- <div id="msg">loading...</div> -->
    <div id="main" class="container">
        <h2 class="page-title">Kohei Uehara's Website</h2>
        <hr />
        <h5>About me</h5>
            I am Kohei Uehara (上原 康平), a Ph. D. student at <a href="https://www.mi.t.u-tokyo.ac.jp/en/">Machine Intelligence Lab</a>, the University of Tokyo.
            I am advised by Prof. Tatsuya Harada. 
            My research interest focuses on machine learning across vision and language.
        <hr />
        <h5>Education</h5>

            <ul>
                <li>April 2020 - Now : Ph. D. student, Information Science and Technology, The University of Tokyo</li>
                <li>April 2018 - March 2020 : Master’s student, Information Science and Technology, The University of Tokyo</li>
                <li>April 2014 - March 2018 : Undergraduate student, Mechano-Informatics, The University of Tokyo</li>
            </ul>
        <hr />
        <h5>Publications</h5>
        <h6>International Conference</h6>
            <ul>
                <li><u>Kohei Uehara</u> and Tatsuya Harada. <strong>K-VQG: Knowledge-aware Visual Question Generation for Common-sense Acquisition</strong>. WACV, 2023 (accepted).
                <a href="https://arxiv.org/abs/2203.07890">[Paper]</a> <a href="https://uehara-mech.github.io/kvqg">[Project Page]</a>
            </li>
                <li><u>Kohei Uehara</u>, Nan Duan, Tatsuya Harada.
                    <strong>Learning to Ask Informative Sub-Questions for Visual Question Answering</strong>.
                    5th Multimodal Learning and Applications Workshop (CVPR 2022, Workshop), 2022.
                    <a href="https://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Uehara_Learning_To_Ask_Informative_Sub-Questions_for_Visual_Question_Answering_CVPRW_2022_paper.html">[Paper]</a>
                </li>
                <li><u>Kohei Uehara</u>†, Yusuke Mori† (†equal contribution), Yusuke Mukuta and Tatsuya Harada.
                    <strong>ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer</strong>.
                    The 1st International Workshop on Multimodal Understanding for the Web and Social Media (WWW 2022, Workshop), 2022.
                    <a href="https://arxiv.org/abs/2202.07305">[Paper]</a></li>
                <li><u>Kohei Uehara</u>, Tatsuya Harada. <strong>Unsupervised Keyword Extraction for Full-sentence VQA.</strong> First International Workshop on Natural Language Processing Beyond Text with EMNLP 2020 (NLPBT2020), 2020. <a href="https://www.aclweb.org/anthology/2020.nlpbt-1.6/">[Paper]</a>
                <li>Sho Maeoki, <u>Kohei Uehara</u>, Tatsuya Harada. <strong>Interactive Video Retrieval with Dialog.</strong> CVPR 2020 Workshop on Multimodal Learning, 2020. <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w56/Maeoki_Interactive_Video_Retrieval_With_Dialog_CVPRW_2020_paper.html">[Paper]</a></li>
                <li><u>Kohei Uehara</u>, Antonio Tejero-de-Pablos, Yoshitaka Ushiku and Tatsuya Harada. <strong>Visual Question Generation for Class
                Acquisition of Unknown Objects.</strong> The 15th European Conference on Computer Vision (ECCV2018), 2018. <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Kohei_Uehara_Visual_Question_Generation_ECCV_2018_paper.html">[Paper]</a></li>
            </ul>
        <h6>Domestic Conference</h6>
        <ul>
            <li>森 友亮†, <u>上原康平</u>† († equal contribution), 原田達也.  <strong>視覚・言語融合 Transformer モデルによる画像からの物語文生成</strong>. CAI+CAI first workshop（言語処理学会第27回年次大会 ワークショップ）, 2021.
                <a href="https://sites.google.com/view/cai-workshop">[Paper]</a>
            </li>
        </ul>
        <h6>Others</h6>
        <ul>
            <li><u>Kohei Uehara</u> and Tatsuya Harada. <strong>Learning by Asking Questions for Knowledge-based Novel Object Recognition</strong>. arXiv, 2022.
                <a href="https://arxiv.org/abs/2210.05879">[Paper]</a>
            </li>
        </ul>
        <hr />
        <h5>Competitions</h5>
            
            <ul>
                <li><strong>The 5th place in the Visual Question Answering (VQA) Challenge 2018 in CVPR2018.</strong> Mikihiro Tanaka, Atsuhiro Noguchi,
                <u>Kohei Uehara</u>, Lisa Kawai, Yoshitaka Ushiku, Tatsuya Harada
                    <a href="https://visualqa.org/challenge_2018.html">[competition page]</a>
                </li>
            </ul>
        <hr />
        <h5>Invited Talks</h5>
            <ul>
                <li><u>Kohei Uehara</u>, Antonio Tejero-de-Pablos, Yoshitaka Ushiku and Tatsuya Harada. Visual Question Generation for
                    Class
                    Acquisition of Unknown Objects. <strong>FIT</strong>, 2019. <a href="https://www.ipsj.or.jp/event/fit/fit2019/">[Program]</a>
                </li>
                <li><u>Kohei Uehara</u>, Antonio Tejero-de-Pablos, Yoshitaka Ushiku and Tatsuya Harada. Visual Question Generation for Class
                Acquisition of Unknown Objects. <strong>MIRU</strong>, 2019. <a href="http://cvim.ipsj.or.jp/MIRU2019/">[Program]</a>
                </li>
                <li><u>Kohei Uehara</u>, Antonio Tejero-de-Pablos, Yoshitaka Ushiku and Tatsuya Harada. Visual Question Generation for Class
                Acquisition of Unknown Objects. <strong>PRMU</strong>, 2019. <a href="https://www.ieice.org/ken/program/index.php?tgs_regid=b89c5c2c61be284ce9fca177314c50a8b6026a6f4868fe70c49186a457bbd58e&tgid=IEICE-PRMU">[Program]</a></li>
            </ul>
        <hr />
        <h5>Work Experiences</h5>
            <ul>
                <li>April 2021 - July 2021: NVIDIA, Research Internship</li>
                <li>February 2019 - April 2019 : LINE Corporation, Machine Learning Engineer, Part time job</li>
                <li>August 2018 : Mercari, Inc. Machine Learning Engineer Internship</li>
            </ul>
        <hr />
        <h5>Grants & Fellowships</h5>
        <ul>
            <li>January 2021 - December 2021 : Microsoft Research Asia Collaborative Research for Ph.D. Student 2021 (D-CORE 2021)</li>
            <li>April 2020 -  (March 2023) : Japan Society for the Promotion of Science (JSPS) Research Fellowship for Young Scientists (DC1)</li>
        </ul>
        <hr />
        <h5>Professional Activities</h5>
        <ul>
            <li>Reviewer : ICCV2021, CVPR2022</li>
            <li>Program Committee : 2nd International Workshop on NLP Beyond Text, NLPBT2021 <a
                href="https://sites.google.com/view/nlpbt-2021">[Link]</a></li>
            <li>Program Committee : 1st International Workshop on NLP Beyond Text, NLPBT2020 <a
                href="https://sites.google.com/view/nlpbt-2020">[Link]</a></li>
            <li>Program Committee : 4th MUltimodal Learning and Applications Workshop, MULA2021 <a
                href="https://mula-workshop.github.io/index_2021.html">[Link]</a></li>
        </ul>
        <hr />
        <h5>Links</h5>
            <a href="https://scholar.google.com/citations?user=yZFVY5cAAAAJ&hl=en">Google Scholar Citations</a>
            <br/>
            <div class="links">
                <a href="https://github.com/uehara-mech" class="link-icon"><i class="fa-brands fa-github-square fa-2x"></i></a>
                <a href="https://twitter.com/oldsea00731" class="link-icon"><i class="fa-brands fa-twitter-square fa-2x"></i></a>
                <a href="https://www.facebook.com/profile.php?id=100028442884186" class="link-icon"><i class="fa-brands fa-facebook-square fa-2x"></i></a>
                <a href="https://www.linkedin.com/in/kohei-uehara-58288b176/" class="link-icon"><i class="fa-brands fa-linkedin fa-2x"></i></a>
            </div>
            
        <hr />
        <div class="update">
            last update: October 13, 2022
        </div>



    </div>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>
</body>

</html>